{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f51fcb4-b3de-4287-989f-7dabd9f18bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/r/ritesh11/.local/perlmutter/pytorch2.3.1/lib/python3.11/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/global/homes/r/ritesh11/.local/perlmutter/pytorch2.3.1/lib/python3.11/site-packages/torchtext/utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "from trainer import sequence_accuracy\n",
    "from data import Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from config import TransformerTestConfig\n",
    "from tokenizer import Tokenizer\n",
    "from fn_utils import get_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85896134-593b-4f92-b6ba-f4de682fdf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special tokens & coressponding ids\n",
    "BOS_IDX, PAD_IDX, EOS_IDX, UNK_IDX, SEP_IDX = 0, 1, 2, 3, 4\n",
    "special_symbols = ['<S>', '<PAD>', '</S>', '<UNK>', '<SEP>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9134eeed-7d00-4952-b454-5e19d5d0d266",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TransformerTestConfig(\n",
    "    model_name=\"transformer_64x2\",\n",
    "    root_dir=\"/pscratch/sd/r/ritesh11/SYMBA_test/Vanilla\",\n",
    "    data_dir=\"/pscratch/sd/r/ritesh11/SYMBA_test/SYMBA_test\",\n",
    "    device='cuda',\n",
    "    embedding_size=512,\n",
    "    nhead=8,\n",
    "    num_encoder_layers=3,\n",
    "    num_decoder_layers=3,\n",
    "    dropout=0.1,\n",
    "    hidden_dim=8192,\n",
    "    src_max_len=280,\n",
    "    tgt_max_len=323,\n",
    "    src_voc_size=None,\n",
    "    tgt_voc_size=None,\n",
    "    seed=42,\n",
    "    to_replace=False,\n",
    "    index_pool_size=200,\n",
    "    momentum_pool_size=200,\n",
    "    is_prefix=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6833360-e75f-4e5f-aeb1-e7065aa17fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(config.data_dir+\"train.csv\")\n",
    "df_test = pd.read_csv(config.data_dir+\"test.csv\")\n",
    "df_valid = pd.read_csv(config.data_dir+\"valid.csv\")\n",
    "\n",
    "df = pd.concat([df_train,df_valid,df_test]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "102dfb6e-96e9-4fe3-8439-ba8ada8e48c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing source vocab: 100%|██████████| 15552/15552 [00:00<00:00, 20614.19it/s]\n",
      "Processing target vocab: 100%|██████████| 15552/15552 [00:01<00:00, 14230.70it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(df, config.index_pool_size,config.momentum_pool_size, special_symbols, UNK_IDX,config.to_replace)\n",
    "src_vocab = tokenizer.build_src_vocab(config.seed)\n",
    "src_itos = {value: key for key, value in src_vocab.get_stoi().items()}\n",
    "tgt_vocab = tokenizer.build_tgt_vocab()\n",
    "tgt_itos = {value: key for key, value in tgt_vocab.get_stoi().items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5c1f9e5-7bc7-4a2a-a79e-ad6f1428d618",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.src_voc_size = len(src_vocab)\n",
    "config.tgt_voc_size = len(tgt_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "164c982f-34d9-4d0c-9ad5-1418e557f8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/r/ritesh11/.local/perlmutter/pytorch2.3.1/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "model = get_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77f167d4-0632-4749-b14f-4ea68b28e0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = Data.get_data(\n",
    "            df_train, df_test, df_valid, config, tokenizer,src_vocab, tgt_vocab)\n",
    "\n",
    "test_ds = datasets['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad6fa91c-b6ad-4c9b-b6eb-526e2c771e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using epoch 20 model for predictions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seq_Acc_Cal:   3%|▎         | 49/1555 [00:22<11:25,  2.20it/s, seq_accuracy=0.898]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43msequence_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtgt_itos\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_ds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/global/u2/r/ritesh11/GSOC_2024/amp/vanilla/trainer.py:34\u001b[0m, in \u001b[0;36msequence_accuracy\u001b[0;34m(config, test_ds, tgt_itos, load_best, epoch, test_size)\u001b[0m\n\u001b[1;32m     32\u001b[0m pbar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeq_Acc_Cal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[0;32m---> 34\u001b[0m     original_tokens, predicted_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest_ds\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrandom_idx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtgt_itos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     original_tokens \u001b[38;5;241m=\u001b[39m original_tokens\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     37\u001b[0m     predicted_tokens \u001b[38;5;241m=\u001b[39m predicted_tokens\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m/global/u2/r/ritesh11/GSOC_2024/amp/vanilla/trainer.py:139\u001b[0m, in \u001b[0;36mPredictor.predict\u001b[0;34m(self, test_example, itos, raw_tokens)\u001b[0m\n\u001b[1;32m    133\u001b[0m src \u001b[38;5;241m=\u001b[39m test_example[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    135\u001b[0m src_mask, tgt_mask, src_padding_mask, tgt_padding_mask \u001b[38;5;241m=\u001b[39m create_mask(\n\u001b[1;32m    136\u001b[0m     src, torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m    137\u001b[0m )\n\u001b[0;32m--> 139\u001b[0m tgt_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgreedy_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_symbol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBOS_IDX\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raw_tokens:\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m test_example[\u001b[38;5;241m1\u001b[39m], tgt_tokens\n",
      "File \u001b[0;32m/global/u2/r/ritesh11/GSOC_2024/amp/vanilla/trainer.py:97\u001b[0m, in \u001b[0;36mPredictor.greedy_decode\u001b[0;34m(self, src, src_mask, src_padding_mask, start_symbol)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03mPerforms greedy decoding to generate predictions.\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;124;03m    Tensor: Generated token sequence.\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     93\u001b[0m src, src_mask, src_padding_mask \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     94\u001b[0m     src\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), src_mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), src_padding_mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     95\u001b[0m )\n\u001b[0;32m---> 97\u001b[0m memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_padding_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     99\u001b[0m ys \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mfill_(start_symbol)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_len):\n",
      "File \u001b[0;32m/global/u2/r/ritesh11/GSOC_2024/amp/vanilla/model.py:135\u001b[0m, in \u001b[0;36mModel.encode\u001b[0;34m(self, src, src_mask, src_pad_mask)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, src: Tensor, src_mask: Tensor, src_pad_mask: Tensor):\n\u001b[1;32m    125\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;124;03m    Encode the source input.\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;124;03m        Tensor: Encoded tensor.\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mencoder(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpositional_encoding\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msrc_tok_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m, src_mask, src_pad_mask)\n",
      "File \u001b[0;32m~/.local/perlmutter/pytorch2.3.1/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/perlmutter/pytorch2.3.1/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/global/u2/r/ritesh11/GSOC_2024/amp/vanilla/model.py:32\u001b[0m, in \u001b[0;36mPositionalEncoding.forward\u001b[0;34m(self, token_embedding)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, token_embedding: Tensor):\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_embedding\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_embedding\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mtoken_embedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/perlmutter/pytorch2.3.1/lib/python3.11/site-packages/torch/nn/modules/module.py:1528\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m             tracing_state\u001b[38;5;241m.\u001b[39mpop_scope()\n\u001b[1;32m   1526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m-> 1528\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrapped_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1529\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "preds = sequence_accuracy(config,test_ds,tgt_itos,test_size=len(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c315dea8-0de3-4c2f-a512-d827d50e5676",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-2.3.1",
   "language": "python",
   "name": "pytorch-2.3.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
