{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be884b73-83ee-45f0-ae8b-6545816a015c",
   "metadata": {},
   "source": [
    "'-1/18*i*e^2*gamma_{%\\\\rho_52180,%eta_21888,%eps_14253}*gamma_{+%\\\\rho_52180,%gam_17939,%del_15647}*d_{j_18804,%gam_17939}(p_4)_u^(*)*d_{i_18804,%del_15647}(p_3)_v*s_{k_18802,%eps_14253}(p_1)_u*s_{l_18802,%eta_21888}(p_2)_v^(*)/(m_s^2 + s_12 + 1/2*reg_prop)' \n",
    "\n",
    "'-1/18*i*e^2*gamma_{\\\\INDEX_0,INDEX_1,INDEX_2}*gamma_{+\\\\INDEX_0,INDEX_3,INDEX_4}*d_{MOMENTUM_0,INDEX_3}(p_4)_u^(*)*d_{MOMENTUM_1,INDEX_4}(p_3)_v*s_{MOMENTUM_2,INDEX_2}(p_1)_u*s_{MOMENTUM_3,INDEX_1}(p_2)_v^(*)/(m_s^2+s_12+1/2*reg_prop)'\n",
    "\n",
    "['-', '1/18', '*', 'i', '*', 'e', '^', '2', '*', 'gamma_{', '\\\\', 'INDEX_0', ',', 'INDEX_1', ',', 'INDEX_2', '}', '*', 'gamma_{', '+', '\\\\', 'INDEX_0', ',', 'INDEX_3', ',', 'INDEX_4', '}', '*', 'd_{', 'MOMENTUM_0', ',', 'INDEX_3', '}', '(', 'p_4', ')', '_u', '^', '(', '*', ')', '*', 'd_{', 'MOMENTUM_1', ',', 'INDEX_4', '}', '(', 'p_3', ')', '_v', '*', 's_{', 'MOMENTUM_2', ',', 'INDEX_2', '}', '(', 'p_1', ')', '_u', '*', 's_{', 'MOMENTUM_3', ',', 'INDEX_1', '}', '(', 'p_2', ')', '_v', '^', '(', '*', ')', '/', '(', 'm_s', '^', '2', '+', 's_12', '+', '1/2', '*', 'reg_prop', ')']\n",
    "\n",
    "\n",
    "'1/324*e^4*(16*m_d^2*m_s^2 + 8*m_d^2*s_12 + 8*s_14*s_23 + 8*s_13*s_24 + 8*m_s^2*s_34)*(m_s^2 + s_12 + 1/2*reg_prop)^(-2)'\n",
    "\n",
    "['1/324', '*', 'e', '^', '4', '*', '(', '16', '*', 'm_d', '^', '2', '*', 'm_s', '^', '2', '+', '8', '*', 'm_d', '^', '2', '*', 's_12', '+', '8', '*', 's_14', '*', 's_23', '+', '8', '*', 's_13', '*', 's_24', '+', '8', '*', 'm_s', '^', '2', '*', 's_34', ')', '*', '(', 'm_s', '^', '2', '+', 's_12', '+', '1/2', '*', 'reg_prop', ')', '^', '(', '-', '2', ')']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f07b230-711f-49b1-9972-aa6548d811b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/r/ritesh11/.local/perlmutter/pytorch2.3.1/lib/python3.11/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/global/homes/r/ritesh11/.local/perlmutter/pytorch2.3.1/lib/python3.11/site-packages/torchtext/utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter, OrderedDict\n",
    "from itertools import cycle\n",
    "import re\n",
    "import random\n",
    "from torchtext.vocab import vocab\n",
    "from tqdm import tqdm\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fd3403c-de9e-4d94-b6c5-8a63fbb8c3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \"\"\"\n",
    "    Tokenizer for processing symbolic mathematical expressions.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, index_token_pool_size, momentum_token_pool_size, special_symbols, UNK_IDX, to_replace):\n",
    "        self.amps = df.amp.tolist()\n",
    "        self.sqamps = df.sqamp.tolist()\n",
    "\n",
    "        # Issue warnings if token pool sizes are too small\n",
    "        if index_token_pool_size < 100:\n",
    "            warnings.warn(f\"Index token pool size ({index_token_pool_size}) is small. Consider increasing it.\", UserWarning)\n",
    "        if momentum_token_pool_size < 100:\n",
    "            warnings.warn(f\"Momentum token pool size ({momentum_token_pool_size}) is small. Consider increasing it.\", UserWarning)\n",
    "        \n",
    "        # Generate token pools\n",
    "        self.tokens_pool = [f\"INDEX_{i}\" for i in range(index_token_pool_size)]\n",
    "        self.momentum_pool = [f\"MOMENTUM_{i}\" for i in range(momentum_token_pool_size)]\n",
    "        \n",
    "        # Regular expression patterns for token replacement\n",
    "        self.pattern_momentum = re.compile(r'\\b[ijkl]_\\d{1,}\\b')\n",
    "        self.pattern_num_123 = re.compile(r'\\b(?![ps]_)\\w+_\\d{1,}\\b')\n",
    "        self.pattern_special = re.compile(r'\\b\\w+_+\\w+\\b\\\\')\n",
    "        self.pattern_underscore_curly = re.compile(r'\\b\\w+_{')\n",
    "        self.pattern_prop = re.compile(r'Prop')\n",
    "        self.pattern_int = re.compile(r'int\\{')\n",
    "        self.pattern_operators = {\n",
    "            '+': re.compile(r'\\+'), '-': re.compile(r'-'), '*': re.compile(r'\\*'),\n",
    "            ',': re.compile(r','), '^': re.compile(r'\\^'), '%': re.compile(r'%'),\n",
    "            '}': re.compile(r'\\}'), '(': re.compile(r'\\('), ')': re.compile(r'\\)')\n",
    "        }\n",
    "        self.pattern_mass = re.compile(r'\\b\\w+_\\w\\b')\n",
    "        self.pattern_s = re.compile(r'\\b\\w+_\\d{2,}\\b')\n",
    "        self.pattern_reg_prop = re.compile(r'\\b\\w+_\\d{1}\\b')\n",
    "        self.pattern_antipart = re.compile(r'(\\w)_\\w+_\\d+\\(X\\)\\^\\(\\*\\)')\n",
    "        self.pattern_part = re.compile(r'(\\w)_\\w+_\\d+\\(X\\)')\n",
    "        self.pattern_index = re.compile(r'\\b\\w+_\\w+_\\d{2,}\\b')\n",
    "        \n",
    "        self.special_symbols = special_symbols\n",
    "        self.UNK_IDX = UNK_IDX\n",
    "        self.to_replace = to_replace\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_whitespace(expression):\n",
    "        \"\"\"Remove all forms of whitespace from the expression.\"\"\"\n",
    "        return re.sub(r'\\s+', '', expression)\n",
    "\n",
    "    @staticmethod\n",
    "    def split_expression(expression):\n",
    "        \"\"\"Split the expression by space delimiter.\"\"\"\n",
    "        return re.split(r' ', expression)\n",
    "\n",
    "    def build_tgt_vocab(self):\n",
    "        \"\"\"Build vocabulary for target sequences.\"\"\"\n",
    "        counter = Counter()\n",
    "        for eqn in tqdm(self.sqamps, desc='Processing target vocab'):\n",
    "            counter.update(self.tgt_tokenize(eqn))\n",
    "        voc = vocab(OrderedDict(counter), specials=self.special_symbols[:], special_first=True)\n",
    "        voc.set_default_index(self.UNK_IDX)\n",
    "        return voc\n",
    "\n",
    "    def build_src_vocab(self, seed):\n",
    "        \"\"\"Build vocabulary for source sequences.\"\"\"\n",
    "        counter = Counter()\n",
    "        for diag in tqdm(self.amps, desc='Processing source vocab'):\n",
    "            counter.update(self.src_tokenize(diag, seed))\n",
    "        voc = vocab(OrderedDict(counter), specials=self.special_symbols[:], special_first=True)\n",
    "        voc.set_default_index(self.UNK_IDX)\n",
    "        return voc\n",
    "    \n",
    "    def src_replace(self, ampl, seed):\n",
    "        \"\"\"Replace indexed and momentum variables with tokenized equivalents.\"\"\"\n",
    "        ampl = self.remove_whitespace(ampl)\n",
    "        \n",
    "        random.seed(seed)\n",
    "        token_cycle = cycle(random.sample(self.tokens_pool, len(self.tokens_pool)))\n",
    "        momentum_cycle = cycle(random.sample(self.momentum_pool, len(self.momentum_pool)))\n",
    "        \n",
    "        # Replace momentum tokens\n",
    "        temp_ampl = ampl\n",
    "        momentum_mapping = {match: next(momentum_cycle) for match in set(self.pattern_momentum.findall(ampl))}\n",
    "        for key, value in momentum_mapping.items():\n",
    "            temp_ampl = temp_ampl.replace(key, value)\n",
    "        \n",
    "        # Replace index tokens\n",
    "        num_123_mapping = {match: next(token_cycle) for match in set(self.pattern_num_123.findall(ampl))}\n",
    "        for key, value in num_123_mapping.items():\n",
    "            temp_ampl = temp_ampl.replace(key, value)\n",
    "\n",
    "        # Replace pattern index tokens\n",
    "        pattern_index_mapping = {match: f\"{'_'.join(match.split('_')[:-1])} {next(token_cycle)}\"\n",
    "                for match in set(self.pattern_index.findall(ampl))\n",
    "            }\n",
    "        for key, value in pattern_index_mapping.items():\n",
    "            temp_ampl = temp_ampl.replace(key, value)\n",
    "            \n",
    "        return temp_ampl\n",
    "    \n",
    "    def src_tokenize(self, ampl, seed):\n",
    "        \"\"\"Tokenize source expression, optionally applying replacements.\"\"\"\n",
    "        temp_ampl = self.src_replace(ampl, seed) if self.to_replace else ampl\n",
    "        temp_ampl = temp_ampl.replace('\\\\\\\\', '\\\\').replace('\\\\', ' \\\\ ').replace('%', '')\n",
    "\n",
    "        temp_ampl = self.pattern_underscore_curly.sub(lambda match: f' {match.group(0)} ', temp_ampl)\n",
    "\n",
    "        \n",
    "        for symbol, pattern in self.pattern_operators.items():\n",
    "            temp_ampl = pattern.sub(f' {symbol} ', temp_ampl)\n",
    "        \n",
    "        temp_ampl = re.sub(r' {2,}', ' ', temp_ampl)\n",
    "        return [token for token in self.split_expression(temp_ampl) if token]\n",
    "\n",
    "    def tgt_tokenize(self, sqampl):\n",
    "        \"\"\"Tokenize target expression.\"\"\"\n",
    "        sqampl = self.remove_whitespace(sqampl)\n",
    "        temp_sqampl = sqampl\n",
    "        \n",
    "        for symbol, pattern in self.pattern_operators.items():\n",
    "            temp_sqampl = pattern.sub(f' {symbol} ', temp_sqampl)\n",
    "        \n",
    "        for pattern in [self.pattern_reg_prop, self.pattern_mass, self.pattern_s]:\n",
    "            temp_sqampl = pattern.sub(lambda match: f' {match.group(0)} ', temp_sqampl)\n",
    "        \n",
    "        temp_sqampl = re.sub(r' {2,}', ' ', temp_sqampl)\n",
    "        return [token for token in self.split_expression(temp_sqampl) if token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5c1b89a-bbbf-48ea-a1d9-6850bc58d798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_to_list(file_path):\n",
    "    lines_list = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            lines_list.append(line.strip())  # .strip() removes newline characters\n",
    "    return lines_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7890c257-4e61-4911-a1c2-5aa9025c83ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_indices(tokenizer, expressions, index_token_pool_size=50, momentum_token_pool_size=50):\n",
    "    # Function to replace indices with a new set of tokens for each expression\n",
    "    def replace_indices(token_list, index_map):\n",
    "        new_index = (f\"INDEX_{i}\" for i in range(index_token_pool_size))  # Local generator for new indices\n",
    "        new_tokens = []\n",
    "        for token in token_list:\n",
    "            if \"INDEX_\" in token:\n",
    "                if token not in index_map:\n",
    "                    try:\n",
    "                        index_map[token] = next(new_index)\n",
    "                    except StopIteration:\n",
    "                        # Handle the case where no more indices are available\n",
    "                        raise ValueError(\"Ran out of unique indices, increase token_pool_size\")\n",
    "                new_tokens.append(index_map[token])\n",
    "            else:\n",
    "                new_tokens.append(token)\n",
    "        return new_tokens\n",
    "\n",
    "    def replace_momenta(token_list, index_map):\n",
    "        new_index = (f\"MOMENTUM_{i}\" for i in range(momentum_token_pool_size))  # Local generator for new indices\n",
    "        new_tokens = []\n",
    "        for token in token_list:\n",
    "            if \"MOMENTUM_\" in token:\n",
    "                if token not in index_map:\n",
    "                    try:\n",
    "                        index_map[token] = next(new_index)\n",
    "                    except StopIteration:\n",
    "                        # Handle the case where no more indices are available\n",
    "                        raise ValueError(\"Ran out of unique indices, increase momentum_token_pool_size\")\n",
    "                new_tokens.append(index_map[token])\n",
    "            else:\n",
    "                new_tokens.append(token)\n",
    "        return new_tokens\n",
    "\n",
    "    normalized_expressions = []\n",
    "    # Replace indices in each expression randomly\n",
    "    for expr in tqdm(expressions,desc=\"Normalizing..\"):\n",
    "        toks = tokenizer.src_tokenize(expr,42)\n",
    "        normalized_expressions.append(replace_momenta(replace_indices(toks, {}), {}))\n",
    "\n",
    "    return normalized_expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19fe3c7e-c8c1-4c2a-8969-c64cbd8b1973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug_data(df):\n",
    "    # Extract columns\n",
    "    amps = df['amp']\n",
    "    sqamps = df['sqamp']\n",
    "\n",
    "    # Data augmentation\n",
    "    n_samples = 1\n",
    "    aug_amps = []\n",
    "\n",
    "    for amp in tqdm(amps, desc='processing'):\n",
    "        random_seed = [random.randint(1, 1000) for _ in range(n_samples)]\n",
    "        for seed in random_seed:\n",
    "            aug_amps.append(tokenizer.src_replace(amp, seed))\n",
    "\n",
    "    aug_sqamps = [sqamp for sqamp in sqamps for _ in range(n_samples)]\n",
    "\n",
    "\n",
    "    normal_amps = normalize_indices(tokenizer,aug_amps,INDEX_POOL_SIZE,MOMENTUM_POOL_SIZE)\n",
    "    \n",
    "    aug_amps = []\n",
    "    for amp in normal_amps:\n",
    "        aug_amps.append(\"\".join(amp))\n",
    "\n",
    "    # Create augmented DataFrame\n",
    "    df_aug = pd.DataFrame({\"amp\": aug_amps, \"sqamp\": aug_sqamps})\n",
    "\n",
    "    return df_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca820c14-3fa7-42b5-9253-dbb81cd183cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dir = \"/pscratch/sd/r/ritesh11/raw_data/SYMBA_test_data\"\n",
    "line_list = [read_file_to_list(os.path.join(curr_dir, file)) for file in os.listdir(curr_dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b23dc1c-ddc9-4416-98da-53a4f98ae129",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "data_train = {'sqamp': [], 'process': [], 'amp': []}\n",
    "data_valid = {'sqamp': [], 'process': [], 'amp': []}\n",
    "data_test = {'sqamp': [], 'process': [], 'amp': []}\n",
    "\n",
    "data_list = []\n",
    "\n",
    "\n",
    "for lines in line_list:\n",
    "    for c in lines:\n",
    "        res = c.split(\" : \")\n",
    "        try:\n",
    "            data_list.append((res[1], res[2], res[3]))\n",
    "        except IndexError:\n",
    "            pass\n",
    "        except Exception:\n",
    "            if len(res) > 2 and \"error\" not in res[2]:\n",
    "                print(res)\n",
    "                break\n",
    "\n",
    "\n",
    "random.shuffle(data_list)\n",
    "\n",
    "total = len(data_list)\n",
    "train_size = int(0.80 * total)\n",
    "valid_size = int(0.10 * total)\n",
    "\n",
    "# Assign data to splits\n",
    "data_train['process'], data_train['amp'], data_train['sqamp'] = zip(*data_list[:train_size])\n",
    "data_valid['process'], data_valid['amp'], data_valid['sqamp'] = zip(*data_list[train_size:train_size + valid_size])\n",
    "data_test['process'], data_test['amp'], data_test['sqamp'] = zip(*data_list[train_size + valid_size:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f07187f-d629-4afc-b0d1-cc4b4d3f9ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special token indices\n",
    "BOS_IDX = 0  # Beginning of Sequence\n",
    "PAD_IDX = 1  # Padding\n",
    "EOS_IDX = 2  # End of Sequence\n",
    "UNK_IDX = 3  # Unknown Token\n",
    "SEP_IDX = 4  # Separator Token\n",
    "\n",
    "# Special token symbols\n",
    "SPECIAL_SYMBOLS = ['<S>', '<PAD>', '</S>', '<UNK>', '<SEP>']\n",
    "\n",
    "INDEX_POOL_SIZE = 200\n",
    "MOMENTUM_POOL_SIZE = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a900b3f-6a97-4e1b-abef-b6374cabf52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(data_train)\n",
    "df_test = pd.DataFrame(data_valid)\n",
    "df_valid = pd.DataFrame(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b8d40d12-d622-4e30-a3f5-79e584701b8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sqamp</th>\n",
       "      <th>process</th>\n",
       "      <th>amp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16/81*e^4*m_s^2*(m_s^2 + 1/4*s_13)*(m_s^2 + -s...</td>\n",
       "      <td>Vertex V_0: OffShell A(X_2),  OffShell s(X_3),...</td>\n",
       "      <td>1/9*i*e^2*(m_s*gamma_{%\\rho_426719,%eta_216031...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1/81*e^4*(16*m_b^2*m_d^2 + 8*m_d^2*s_12 + 8*s_...</td>\n",
       "      <td>Vertex V_0:AntiPart d(X_3), d(X_4),  OffShell ...</td>\n",
       "      <td>-1/9*i*e^2*gamma_{+%\\tau_56188,%gam_26119,%eta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1/9*e^4*(16*m_b^2*m_e^2 + 8*m_b^2*s_12 + 8*s_1...</td>\n",
       "      <td>Vertex V_1:AntiPart b(X_3),  OffShell b(X_4), ...</td>\n",
       "      <td>-1/3*i*e^2*gamma_{+%\\tau_82943,%gam_57488,%del...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e^4*(16*m_e^2*m_mu^2 + (-8)*m_mu^2*s_13 + 8*s_...</td>\n",
       "      <td>Vertex V_1: OffShell mu(X_2),  OffShell mu(X_4...</td>\n",
       "      <td>-i*e^2*gamma_{+%\\sigma_158360,%eta_49850,%eps_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1/36*e^4*(16*m_b^2*m_t^2 + 8*m_b^2*s_12 + 8*s_...</td>\n",
       "      <td>Vertex V_1:AntiPart b(X_3), b(X_4),  OffShell ...</td>\n",
       "      <td>-1/6*i*e^2*gamma_{+%\\tau_98491,%gam_57008,%del...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12436</th>\n",
       "      <td>2*e^4*s_23*s_24*(s_23 + -1/2*reg_prop)^(-2) + ...</td>\n",
       "      <td>Vertex V_0:A(X_2), e(X_3), AntiPart  OffShell ...</td>\n",
       "      <td>-1/2*i*e^2*(p_2_%\\mu_6398*gamma_{+%\\mu_6398,%g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12437</th>\n",
       "      <td>4/9*e^4*(16*m_e^2*m_tt^2 + 8*m_tt^2*s_12 + 8*s...</td>\n",
       "      <td>Vertex V_1:AntiPart  OffShell tt(X_3),  OffShe...</td>\n",
       "      <td>2/3*i*e^2*gamma_{+%\\sigma_82763,%gam_47766,%de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12438</th>\n",
       "      <td>e^4*(16*m_e^4 + (-8)*m_e^2*s_13 + 8*s_14*s_23 ...</td>\n",
       "      <td>Vertex V_1:e(X_2),  OffShell e(X_4),  OffShell...</td>\n",
       "      <td>i*e^2*gamma_{+%\\sigma_7873,%gam_10888,%del_741...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12439</th>\n",
       "      <td>2/81*e^4*s_23*s_24*(s_13 + -1/2*reg_prop)^(-2)...</td>\n",
       "      <td>Vertex V_0:A(X_2), s(X_4), AntiPart  OffShell ...</td>\n",
       "      <td>-1/18*i*e^2*(p_2_%\\rho_133929*gamma_{%\\rho_133...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12440</th>\n",
       "      <td>-1/324*e^4*s_33*(64*m_d^2 + (-16)*s_34)*(m_d^2...</td>\n",
       "      <td>Vertex V_0:A(X_2),  OffShell d(X_3), AntiPart ...</td>\n",
       "      <td>1/9*i*e^2*(m_d*gamma_{%\\nu_116729,%gam_335596,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12441 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   sqamp  \\\n",
       "0      16/81*e^4*m_s^2*(m_s^2 + 1/4*s_13)*(m_s^2 + -s...   \n",
       "1      1/81*e^4*(16*m_b^2*m_d^2 + 8*m_d^2*s_12 + 8*s_...   \n",
       "2      1/9*e^4*(16*m_b^2*m_e^2 + 8*m_b^2*s_12 + 8*s_1...   \n",
       "3      e^4*(16*m_e^2*m_mu^2 + (-8)*m_mu^2*s_13 + 8*s_...   \n",
       "4      1/36*e^4*(16*m_b^2*m_t^2 + 8*m_b^2*s_12 + 8*s_...   \n",
       "...                                                  ...   \n",
       "12436  2*e^4*s_23*s_24*(s_23 + -1/2*reg_prop)^(-2) + ...   \n",
       "12437  4/9*e^4*(16*m_e^2*m_tt^2 + 8*m_tt^2*s_12 + 8*s...   \n",
       "12438  e^4*(16*m_e^4 + (-8)*m_e^2*s_13 + 8*s_14*s_23 ...   \n",
       "12439  2/81*e^4*s_23*s_24*(s_13 + -1/2*reg_prop)^(-2)...   \n",
       "12440  -1/324*e^4*s_33*(64*m_d^2 + (-16)*s_34)*(m_d^2...   \n",
       "\n",
       "                                                 process  \\\n",
       "0      Vertex V_0: OffShell A(X_2),  OffShell s(X_3),...   \n",
       "1      Vertex V_0:AntiPart d(X_3), d(X_4),  OffShell ...   \n",
       "2      Vertex V_1:AntiPart b(X_3),  OffShell b(X_4), ...   \n",
       "3      Vertex V_1: OffShell mu(X_2),  OffShell mu(X_4...   \n",
       "4      Vertex V_1:AntiPart b(X_3), b(X_4),  OffShell ...   \n",
       "...                                                  ...   \n",
       "12436  Vertex V_0:A(X_2), e(X_3), AntiPart  OffShell ...   \n",
       "12437  Vertex V_1:AntiPart  OffShell tt(X_3),  OffShe...   \n",
       "12438  Vertex V_1:e(X_2),  OffShell e(X_4),  OffShell...   \n",
       "12439  Vertex V_0:A(X_2), s(X_4), AntiPart  OffShell ...   \n",
       "12440  Vertex V_0:A(X_2),  OffShell d(X_3), AntiPart ...   \n",
       "\n",
       "                                                     amp  \n",
       "0      1/9*i*e^2*(m_s*gamma_{%\\rho_426719,%eta_216031...  \n",
       "1      -1/9*i*e^2*gamma_{+%\\tau_56188,%gam_26119,%eta...  \n",
       "2      -1/3*i*e^2*gamma_{+%\\tau_82943,%gam_57488,%del...  \n",
       "3      -i*e^2*gamma_{+%\\sigma_158360,%eta_49850,%eps_...  \n",
       "4      -1/6*i*e^2*gamma_{+%\\tau_98491,%gam_57008,%del...  \n",
       "...                                                  ...  \n",
       "12436  -1/2*i*e^2*(p_2_%\\mu_6398*gamma_{+%\\mu_6398,%g...  \n",
       "12437  2/3*i*e^2*gamma_{+%\\sigma_82763,%gam_47766,%de...  \n",
       "12438  i*e^2*gamma_{+%\\sigma_7873,%gam_10888,%del_741...  \n",
       "12439  -1/18*i*e^2*(p_2_%\\rho_133929*gamma_{%\\rho_133...  \n",
       "12440  1/9*i*e^2*(m_d*gamma_{%\\nu_116729,%gam_335596,...  \n",
       "\n",
       "[12441 rows x 3 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9bd92a3-56b9-492e-a978-18d1a38cd8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(_, 500, 500, _, _, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "16c772e1-1122-45a8-bae2-d16cf5120da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing: 100%|██████████| 12441/12441 [00:03<00:00, 3366.41it/s]\n",
      "Normalizing..: 100%|██████████| 12441/12441 [00:00<00:00, 15237.99it/s]\n",
      "processing: 100%|██████████| 1556/1556 [00:00<00:00, 3441.24it/s]\n",
      "Normalizing..: 100%|██████████| 1556/1556 [00:00<00:00, 16753.79it/s]\n",
      "processing: 100%|██████████| 1555/1555 [00:00<00:00, 3308.95it/s]\n",
      "Normalizing..: 100%|██████████| 1555/1555 [00:00<00:00, 15381.14it/s]\n"
     ]
    }
   ],
   "source": [
    "df_train = aug_data(df_train)\n",
    "df_valid = aug_data(df_valid)\n",
    "df_test = aug_data(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8a942639-d932-43e2-9830-3a82c33c87ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12441, 2), (1556, 2), (1555, 2))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape, df_valid.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "058d9f77-b3ca-4c45-b55a-2fe508a4a506",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(\"SYMBA_testtrain.csv\",index=False)\n",
    "df_test.to_csv(\"SYMBA_testtest.csv\",index=False)\n",
    "df_valid.to_csv(\"SYMBA_testvalid.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a3acaa2-9fee-4d75-ab61-f789180c356f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_small = pd.read_csv(\"EW_2-3train.csv\")\n",
    "df_test_small = pd.read_csv(\"EW_2-3test.csv\")\n",
    "df_valid_small = pd.read_csv(\"EW_2-3valid.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "961c5825-0b06-496c-8c00-802b686a66cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df_train,df_train_small]).sample(frac=1).to_csv(\"EW_2-2-3train.csv\",index=False)\n",
    "pd.concat([df_test,df_test_small]).sample(frac=1).to_csv(\"EW_2-2-3test.csv\",index=False)\n",
    "pd.concat([df_valid,df_valid_small]).sample(frac=1).to_csv(\"EW_2-2-3valid.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fed00b-bedd-4e8b-837d-0b5b6285d438",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-2.3.1",
   "language": "python",
   "name": "pytorch-2.3.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
