{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d06e7fb-014d-46d9-9b36-e4bd187bf5b0",
   "metadata": {},
   "source": [
    "# Common Task 1.2 Dataset preprocessing\n",
    "The initial task focused on data processing, particularly tokenization, with custom strategies for both source features and target equations. **For source sequences**, amplitude preprocessing involved replacing indexed and momentum variables with predefined token pools for consistency and vocabulary limitation. Operators and symbols were then isolated and tokenized sequentially, with tokens reordered numerically to enhance compatibility with vocabulary-based modeling while preserving index positioning within equations. **For target sequences**, squared amplitude tokenization was more straightforward, involving the identification and isolation of symbols, operators, regular propagators, and masses while maintaining mathematical relationships and representational consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be884b73-83ee-45f0-ae8b-6545816a015c",
   "metadata": {},
   "source": [
    "## Source (Amplitude) and Target (Squared Amplitude) Sequences Example  \n",
    "#### Before and After Processing and Tokenization  \n",
    "\n",
    "### **Raw Source Sequence**  \n",
    "$$\n",
    "-\\frac{1}{18} i e^2 \\gamma_{\\rho_{52180}, \\eta_{21888}, \\epsilon_{14253}} \n",
    "\\gamma_{+\\rho_{52180}, \\gamma_{17939}, \\delta_{15647}} \n",
    "d_{j_{18804}, \\gamma_{17939}}(p_4)_u^{(*)} \n",
    "d_{i_{18804}, \\delta_{15647}}(p_3)_v \n",
    "s_{k_{18802}, \\epsilon_{14253}}(p_1)_u \n",
    "s_{l_{18802}, \\eta_{21888}}(p_2)_v^{(*)} \n",
    "\\Big/ \\left(m_s^2 + s_{12} + \\frac{1}{2} \\text{reg\\_prop} \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "### **Replaced Indices**  \n",
    "$$\n",
    "-\\frac{1}{18} i e^2 \\gamma_{\\text{INDEX}_0, \\text{INDEX}_1, \\text{INDEX}_2} \n",
    "\\gamma_{+\\text{INDEX}_0, \\text{INDEX}_3, \\text{INDEX}_4} \n",
    "d_{\\text{MOMENTUM}_0, \\text{INDEX}_3}(p_4)_u^{(*)} \n",
    "d_{\\text{MOMENTUM}_1, \\text{INDEX}_4}(p_3)_v \n",
    "s_{\\text{MOMENTUM}_2, \\text{INDEX}_2}(p_1)_u \n",
    "s_{\\text{MOMENTUM}_3, \\text{INDEX}_1}(p_2)_v^{(*)} \n",
    "\\Big/ \\left(m_s^2 + s_{12} + \\frac{1}{2} \\text{reg\\_prop} \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "### **Source Tokens**  \n",
    "```text\n",
    "['-', '1/18', '*', 'i', '*', 'e', '^', '2', '*', 'gamma_{', '\\\\', 'INDEX_0', ',', \n",
    "'INDEX_1', ',', 'INDEX_2', '}', '*', 'gamma_{', '+', '\\\\', 'INDEX_0', ',', 'INDEX_3', \n",
    "',', 'INDEX_4', '}', '*', 'd_{', 'MOMENTUM_0', ',', 'INDEX_3', '}', '(', 'p_4', ')', \n",
    "'_u', '^', '(', '*', ')', '*', 'd_{', 'MOMENTUM_1', ',', 'INDEX_4', '}', '(', 'p_3', \n",
    "')', '_v', '*', 's_{', 'MOMENTUM_2', ',', 'INDEX_2', '}', '(', 'p_1', ')', '_u', '*', \n",
    "'s_{', 'MOMENTUM_3', ',', 'INDEX_1', '}', '(', 'p_2', ')', '_v', '^', '(', '*', ')', \n",
    "'/', '(', 'm_s', '^', '2', '+', 's_12', '+', '1/2', '*', 'reg_prop', ')']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Raw Target Sequence**  \n",
    "$$\n",
    "\\frac{1}{324} e^4 \\left( 16 m_d^2 m_s^2 + 8 m_d^2 s_{12} + 8 s_{14} s_{23} + 8 s_{13} s_{24} + \n",
    "8 m_s^2 s_{34} \\right) \\left( m_s^2 + s_{12} + \\frac{1}{2} \\text{reg\\_prop} \\right)^{-2}\n",
    "$$\n",
    "\n",
    "\n",
    "### **Target Tokens**  \n",
    "```text\n",
    "['1/324', '*', 'e', '^', '4', '*', '(', '16', '*', 'm_d', '^', '2', '*', 'm_s', '^', \n",
    "'2', '+', '8', '*', 'm_d', '^', '2', '*', 's_12', '+', '8', '*', 's_14', '*', 's_23', \n",
    "'+', '8', '*', 's_13', '*', 's_24', '+', '8', '*', 'm_s', '^', '2', '*', 's_34', ')', \n",
    "'*', '(', 'm_s', '^', '2', '+', 's_12', '+', '1/2', '*', 'reg_prop', ')', '^', '(', \n",
    "'-', '2', ')']\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f07b230-711f-49b1-9972-aa6548d811b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter, OrderedDict\n",
    "from itertools import cycle\n",
    "import re\n",
    "import random\n",
    "from torchtext.vocab import vocab\n",
    "from tqdm import tqdm\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd3403c-de9e-4d94-b6c5-8a63fbb8c3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \"\"\"\n",
    "    Tokenizer for processing symbolic mathematical expressions.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, index_token_pool_size, momentum_token_pool_size, special_symbols, UNK_IDX, to_replace):\n",
    "        self.amps = df.amp.tolist()\n",
    "        self.sqamps = df.sqamp.tolist()\n",
    "\n",
    "        # Issue warnings if token pool sizes are too small\n",
    "        if index_token_pool_size < 100:\n",
    "            warnings.warn(f\"Index token pool size ({index_token_pool_size}) is small. Consider increasing it.\", UserWarning)\n",
    "        if momentum_token_pool_size < 100:\n",
    "            warnings.warn(f\"Momentum token pool size ({momentum_token_pool_size}) is small. Consider increasing it.\", UserWarning)\n",
    "        \n",
    "        # Generate token pools\n",
    "        self.tokens_pool = [f\"INDEX_{i}\" for i in range(index_token_pool_size)]\n",
    "        self.momentum_pool = [f\"MOMENTUM_{i}\" for i in range(momentum_token_pool_size)]\n",
    "        \n",
    "        # Regular expression patterns for token replacement\n",
    "        self.pattern_momentum = re.compile(r'\\b[ijkl]_\\d{1,}\\b')\n",
    "        self.pattern_num_123 = re.compile(r'\\b(?![ps]_)\\w+_\\d{1,}\\b')\n",
    "        self.pattern_special = re.compile(r'\\b\\w+_+\\w+\\b\\\\')\n",
    "        self.pattern_underscore_curly = re.compile(r'\\b\\w+_{')\n",
    "        self.pattern_prop = re.compile(r'Prop')\n",
    "        self.pattern_int = re.compile(r'int\\{')\n",
    "        self.pattern_operators = {\n",
    "            '+': re.compile(r'\\+'), '-': re.compile(r'-'), '*': re.compile(r'\\*'),\n",
    "            ',': re.compile(r','), '^': re.compile(r'\\^'), '%': re.compile(r'%'),\n",
    "            '}': re.compile(r'\\}'), '(': re.compile(r'\\('), ')': re.compile(r'\\)')\n",
    "        }\n",
    "        self.pattern_mass = re.compile(r'\\b\\w+_\\w\\b')\n",
    "        self.pattern_s = re.compile(r'\\b\\w+_\\d{2,}\\b')\n",
    "        self.pattern_reg_prop = re.compile(r'\\b\\w+_\\d{1}\\b')\n",
    "        self.pattern_antipart = re.compile(r'(\\w)_\\w+_\\d+\\(X\\)\\^\\(\\*\\)')\n",
    "        self.pattern_part = re.compile(r'(\\w)_\\w+_\\d+\\(X\\)')\n",
    "        self.pattern_index = re.compile(r'\\b\\w+_\\w+_\\d{2,}\\b')\n",
    "        \n",
    "        self.special_symbols = special_symbols\n",
    "        self.UNK_IDX = UNK_IDX\n",
    "        self.to_replace = to_replace\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_whitespace(expression):\n",
    "        \"\"\"Remove all forms of whitespace from the expression.\"\"\"\n",
    "        return re.sub(r'\\s+', '', expression)\n",
    "\n",
    "    @staticmethod\n",
    "    def split_expression(expression):\n",
    "        \"\"\"Split the expression by space delimiter.\"\"\"\n",
    "        return re.split(r' ', expression)\n",
    "\n",
    "    def build_tgt_vocab(self):\n",
    "        \"\"\"Build vocabulary for target sequences.\"\"\"\n",
    "        counter = Counter()\n",
    "        for eqn in tqdm(self.sqamps, desc='Processing target vocab'):\n",
    "            counter.update(self.tgt_tokenize(eqn))\n",
    "        voc = vocab(OrderedDict(counter), specials=self.special_symbols[:], special_first=True)\n",
    "        voc.set_default_index(self.UNK_IDX)\n",
    "        return voc\n",
    "\n",
    "    def build_src_vocab(self, seed):\n",
    "        \"\"\"Build vocabulary for source sequences.\"\"\"\n",
    "        counter = Counter()\n",
    "        for diag in tqdm(self.amps, desc='Processing source vocab'):\n",
    "            counter.update(self.src_tokenize(diag, seed))\n",
    "        voc = vocab(OrderedDict(counter), specials=self.special_symbols[:], special_first=True)\n",
    "        voc.set_default_index(self.UNK_IDX)\n",
    "        return voc\n",
    "    \n",
    "    def src_replace(self, ampl, seed):\n",
    "        \"\"\"Replace indexed and momentum variables with tokenized equivalents.\"\"\"\n",
    "        ampl = self.remove_whitespace(ampl)\n",
    "        \n",
    "        random.seed(seed)\n",
    "        token_cycle = cycle(random.sample(self.tokens_pool, len(self.tokens_pool)))\n",
    "        momentum_cycle = cycle(random.sample(self.momentum_pool, len(self.momentum_pool)))\n",
    "        \n",
    "        # Replace momentum tokens\n",
    "        temp_ampl = ampl\n",
    "        momentum_mapping = {match: next(momentum_cycle) for match in set(self.pattern_momentum.findall(ampl))}\n",
    "        for key, value in momentum_mapping.items():\n",
    "            temp_ampl = temp_ampl.replace(key, value)\n",
    "        \n",
    "        # Replace index tokens\n",
    "        num_123_mapping = {match: next(token_cycle) for match in set(self.pattern_num_123.findall(ampl))}\n",
    "        for key, value in num_123_mapping.items():\n",
    "            temp_ampl = temp_ampl.replace(key, value)\n",
    "\n",
    "        # Replace pattern index tokens\n",
    "        pattern_index_mapping = {match: f\"{'_'.join(match.split('_')[:-1])} {next(token_cycle)}\"\n",
    "                for match in set(self.pattern_index.findall(ampl))\n",
    "            }\n",
    "        for key, value in pattern_index_mapping.items():\n",
    "            temp_ampl = temp_ampl.replace(key, value)\n",
    "            \n",
    "        return temp_ampl\n",
    "    \n",
    "    def src_tokenize(self, ampl, seed):\n",
    "        \"\"\"Tokenize source expression, optionally applying replacements.\"\"\"\n",
    "        temp_ampl = self.src_replace(ampl, seed) if self.to_replace else ampl\n",
    "        temp_ampl = temp_ampl.replace('\\\\\\\\', '\\\\').replace('\\\\', ' \\\\ ').replace('%', '')\n",
    "\n",
    "        temp_ampl = self.pattern_underscore_curly.sub(lambda match: f' {match.group(0)} ', temp_ampl)\n",
    "\n",
    "        \n",
    "        for symbol, pattern in self.pattern_operators.items():\n",
    "            temp_ampl = pattern.sub(f' {symbol} ', temp_ampl)\n",
    "        \n",
    "        temp_ampl = re.sub(r' {2,}', ' ', temp_ampl)\n",
    "        return [token for token in self.split_expression(temp_ampl) if token]\n",
    "\n",
    "    def tgt_tokenize(self, sqampl):\n",
    "        \"\"\"Tokenize target expression.\"\"\"\n",
    "        sqampl = self.remove_whitespace(sqampl)\n",
    "        temp_sqampl = sqampl\n",
    "        \n",
    "        for symbol, pattern in self.pattern_operators.items():\n",
    "            temp_sqampl = pattern.sub(f' {symbol} ', temp_sqampl)\n",
    "        \n",
    "        for pattern in [self.pattern_reg_prop, self.pattern_mass, self.pattern_s]:\n",
    "            temp_sqampl = pattern.sub(lambda match: f' {match.group(0)} ', temp_sqampl)\n",
    "        \n",
    "        temp_sqampl = re.sub(r' {2,}', ' ', temp_sqampl)\n",
    "        return [token for token in self.split_expression(temp_sqampl) if token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c1b89a-bbbf-48ea-a1d9-6850bc58d798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_to_list(file_path):\n",
    "    lines_list = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            lines_list.append(line.strip())  # .strip() removes newline characters\n",
    "    return lines_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7890c257-4e61-4911-a1c2-5aa9025c83ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_indices(tokenizer, expressions, index_token_pool_size=50, momentum_token_pool_size=50):\n",
    "    \"\"\"\n",
    "    Normalize index and momentum tokens in a list of mathematical expressions.\n",
    "\n",
    "    This function replaces indexed terms (e.g., \"INDEX_x\", \"MOMENTUM_y\") with a \n",
    "    standardized sequence of tokens. It ensures that each unique token in an \n",
    "    expression is replaced consistently within that expression while avoiding collisions.\n",
    "\n",
    "    Args:\n",
    "        tokenizer: A tokenizer object with a `src_tokenize` method to tokenize expressions.\n",
    "        expressions (list of str): List of mathematical expressions to be normalized.\n",
    "        index_token_pool_size (int, optional): Maximum number of unique index tokens available. Defaults to 50.\n",
    "        momentum_token_pool_size (int, optional): Maximum number of unique momentum tokens available. Defaults to 50.\n",
    "\n",
    "    Returns:\n",
    "        list of str: A list of normalized expressions with replaced index and momentum tokens.\n",
    "    \"\"\"\n",
    "    \n",
    "    def replace_indices(token_list, index_map):\n",
    "        \"\"\"\n",
    "        Replace index tokens in a tokenized expression.\n",
    "\n",
    "        Args:\n",
    "            token_list (list of str): List of tokens from an expression.\n",
    "            index_map (dict): Dictionary mapping original index tokens to new standardized ones.\n",
    "\n",
    "        Returns:\n",
    "            list of str: Token list with replaced index tokens.\n",
    "        \"\"\"\n",
    "        new_index = (f\"INDEX_{i}\" for i in range(index_token_pool_size))  # Generator for unique index tokens\n",
    "        new_tokens = []\n",
    "        \n",
    "        for token in token_list:\n",
    "            if \"INDEX_\" in token:\n",
    "                if token not in index_map:\n",
    "                    try:\n",
    "                        index_map[token] = next(new_index)\n",
    "                    except StopIteration:\n",
    "                        raise ValueError(\"Ran out of unique indices, increase index_token_pool_size\")\n",
    "                new_tokens.append(index_map[token])\n",
    "            else:\n",
    "                new_tokens.append(token)\n",
    "        \n",
    "        return new_tokens\n",
    "\n",
    "    def replace_momenta(token_list, index_map):\n",
    "        \"\"\"\n",
    "        Replace momentum tokens in a tokenized expression.\n",
    "\n",
    "        Args:\n",
    "            token_list (list of str): List of tokens from an expression.\n",
    "            index_map (dict): Dictionary mapping original momentum tokens to new standardized ones.\n",
    "\n",
    "        Returns:\n",
    "            list of str: Token list with replaced momentum tokens.\n",
    "        \"\"\"\n",
    "        new_index = (f\"MOMENTUM_{i}\" for i in range(momentum_token_pool_size))  # Generator for unique momentum tokens\n",
    "        new_tokens = []\n",
    "        \n",
    "        for token in token_list:\n",
    "            if \"MOMENTUM_\" in token:\n",
    "                if token not in index_map:\n",
    "                    try:\n",
    "                        index_map[token] = next(new_index)\n",
    "                    except StopIteration:\n",
    "                        raise ValueError(\"Ran out of unique momenta, increase momentum_token_pool_size\")\n",
    "                new_tokens.append(index_map[token])\n",
    "            else:\n",
    "                new_tokens.append(token)\n",
    "        \n",
    "        return new_tokens\n",
    "\n",
    "    normalized_expressions = []\n",
    "    \n",
    "    # Normalize each expression by replacing indices and momenta\n",
    "    for expr in tqdm(expressions, desc=\"Normalizing...\"):\n",
    "        tokens = tokenizer.src_tokenize(expr, 42)\n",
    "        normalized_expr = replace_momenta(replace_indices(tokens, {}), {})\n",
    "        normalized_expressions.append(normalized_expr)\n",
    "\n",
    "    return normalized_expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fe3c7e-c8c1-4c2a-8969-c64cbd8b1973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug_data(df):\n",
    "    \"\"\"\n",
    "    Perform data augmentation on amplitude expressions and normalize indices.\n",
    "\n",
    "    This function augments the `amp` (amplitude) column by applying token replacement \n",
    "    with different random seeds. It then normalizes indices and momentum terms \n",
    "    to ensure consistent tokenization. The corresponding `sqamp` (squared amplitude) \n",
    "    values are duplicated accordingly.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame containing 'amp' and 'sqamp' columns.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Augmented DataFrame with new 'amp' and corresponding 'sqamp' values.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract amplitude and squared amplitude columns\n",
    "    amps = df[\"amp\"]\n",
    "    sqamps = df[\"sqamp\"]\n",
    "\n",
    "    # Number of augmented samples per original entry\n",
    "    n_samples = 1\n",
    "    aug_amps = []\n",
    "\n",
    "    # Generate augmented amplitude expressions\n",
    "    for amp in tqdm(amps, desc=\"Processing amplitudes\"):\n",
    "        random_seeds = [random.randint(1, 1000) for _ in range(n_samples)]\n",
    "        for seed in random_seeds:\n",
    "            aug_amps.append(tokenizer.src_replace(amp, seed))\n",
    "\n",
    "    # Duplicate squared amplitude values to match augmented amplitude samples\n",
    "    aug_sqamps = [sqamp for sqamp in sqamps for _ in range(n_samples)]\n",
    "\n",
    "    # Normalize indices and momentum tokens in the augmented expressions\n",
    "    normalized_amps = normalize_indices(tokenizer, aug_amps, INDEX_POOL_SIZE, MOMENTUM_POOL_SIZE)\n",
    "\n",
    "    # Convert tokenized expressions back to string format\n",
    "    aug_amps = [\"\".join(amp) for amp in normalized_amps]\n",
    "\n",
    "    # Create a new DataFrame with augmented amplitudes and squared amplitudes\n",
    "    df_aug = pd.DataFrame({\"amp\": aug_amps, \"sqamp\": aug_sqamps})\n",
    "\n",
    "    return df_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca820c14-3fa7-42b5-9253-dbb81cd183cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dir = \"/pscratch/sd/r/ritesh11/raw_data/SYMBA_test_data\"\n",
    "line_list = [read_file_to_list(os.path.join(curr_dir, file)) for file in os.listdir(curr_dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b23dc1c-ddc9-4416-98da-53a4f98ae129",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "data_train = {'sqamp': [], 'process': [], 'amp': []}\n",
    "data_valid = {'sqamp': [], 'process': [], 'amp': []}\n",
    "data_test = {'sqamp': [], 'process': [], 'amp': []}\n",
    "\n",
    "data_list = []  \n",
    "\n",
    "# Extract relevant data from each line\n",
    "for lines in line_list:\n",
    "    for c in lines:\n",
    "        res = c.split(\" : \")  # Split line into components\n",
    "        try:\n",
    "            data_list.append((res[1], res[2], res[3]))  # Store as (process, amp, sqamp)\n",
    "        except IndexError:\n",
    "            pass  \n",
    "        except Exception:\n",
    "            if len(res) > 2 and \"error\" not in res[2]:\n",
    "                print(res)  \n",
    "                break\n",
    "\n",
    "# Shuffle data to ensure randomness in train/valid/test splits\n",
    "random.shuffle(data_list)\n",
    "\n",
    "# Compute split sizes\n",
    "total = len(data_list)\n",
    "train_size = int(0.80 * total)  # 80% for training\n",
    "valid_size = int(0.10 * total)  # 10% for validation\n",
    "\n",
    "# Assign data to splits\n",
    "data_train['process'], data_train['amp'], data_train['sqamp'] = zip(*data_list[:train_size])\n",
    "data_valid['process'], data_valid['amp'], data_valid['sqamp'] = zip(*data_list[train_size:train_size + valid_size])\n",
    "data_test['process'], data_test['amp'], data_test['sqamp'] = zip(*data_list[train_size + valid_size:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f07187f-d629-4afc-b0d1-cc4b4d3f9ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special token indices\n",
    "BOS_IDX = 0  # Beginning of Sequence\n",
    "PAD_IDX = 1  # Padding\n",
    "EOS_IDX = 2  # End of Sequence\n",
    "UNK_IDX = 3  # Unknown Token\n",
    "SEP_IDX = 4  # Separator Token\n",
    "\n",
    "# Special token symbols\n",
    "SPECIAL_SYMBOLS = ['<S>', '<PAD>', '</S>', '<UNK>', '<SEP>']\n",
    "\n",
    "INDEX_POOL_SIZE = 200\n",
    "MOMENTUM_POOL_SIZE = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a900b3f-6a97-4e1b-abef-b6374cabf52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(data_train)\n",
    "df_test = pd.DataFrame(data_valid)\n",
    "df_valid = pd.DataFrame(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d40d12-d622-4e30-a3f5-79e584701b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bd92a3-56b9-492e-a978-18d1a38cd8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(_, 500, 500, _, _, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c772e1-1122-45a8-bae2-d16cf5120da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = aug_data(df_train)\n",
    "df_valid = aug_data(df_valid)\n",
    "df_test = aug_data(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a942639-d932-43e2-9830-3a82c33c87ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape, df_valid.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058d9f77-b3ca-4c45-b55a-2fe508a4a506",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(\"SYMBA_testtrain.csv\",index=False)\n",
    "df_test.to_csv(\"SYMBA_testtest.csv\",index=False)\n",
    "df_valid.to_csv(\"SYMBA_testvalid.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
